{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BiLSTM_Attention",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXFK2b-5wzTy",
        "outputId": "2c263321-6e8a-4d5b-e3de-4eceacf6dd13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "from tensorflow.keras.layers import Layer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import  keras.backend as K\n",
        "\n",
        "class Attention(Layer):\n",
        "\n",
        "  def __init__(self ,   return_sequences):\n",
        "    super(Attention,self).__init__()\n",
        "    self.return_sequences = return_sequences\n",
        "     \n",
        "  \n",
        "  def build(self,input_shape) :  \n",
        "\n",
        "    w_q_init  = tf.random_normal_initializer()\n",
        "    w_k_init = tf.random_normal_initializer()\n",
        "    w_v_init = tf.random_normal_initializer()\n",
        "\n",
        "    embedding_size  = input_shape[-1]\n",
        "\n",
        "    # print(embedding_size)\n",
        "    self.w_q  = tf.Variable(name = 'query ' , initial_value=w_q_init(shape= (embedding_size , 36)), trainable =  True)\n",
        "    self.w_k = tf.Variable(name = 'key ' , initial_value=w_k_init(shape= (embedding_size , 36)), trainable =  True )\n",
        "    self.w_v = tf.Variable(name = 'value ' , initial_value=w_v_init(shape= (embedding_size , 36)), trainable =  True )\n",
        "\n",
        "    # Shape of the subarrays formed is  (embedding_size , self.inputs)\n",
        "    super(Attention , self).build(input_shape)\n",
        "\n",
        "  def call(self , input_sequence) : \n",
        "     \n",
        "    #  print(input_sequence[0])\n",
        "     print(self.w_q.shape , self.w_k.shape  , input_sequence.shape)\n",
        "     \n",
        "     self.q = K.dot(input_sequence , self.w_q)\n",
        "     self.k = K.dot(input_sequence , self.w_k)\n",
        "     self.v = K.dot(input_sequence , self.w_v) \n",
        "    \n",
        "    #  print(self.q.shape , self.k.shape,self.v.shape)\n",
        "\n",
        "     dim_k = self.k.shape[-1]\n",
        "\n",
        "     t  = tf.matmul(self.q, tf.transpose(self.k , perm = [0,2,1]))\n",
        "     \n",
        "     return tf.matmul(K.softmax(tf.matmul(self.q, tf.transpose(self.k , perm = [0,2,1]))/6), self.v)\n",
        "\n",
        "def get_config(self):\n",
        "        return super(Attention,self).get_config()\n"
      ],
      "metadata": {
        "id": "3Ycn2e8fwP7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.datasets import imdb \n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Embedding , Bidirectional , LSTM , Dropout, Dense,TimeDistributed,Flatten\n",
        "import numpy as np\n",
        "\n",
        "data = open(\"book.txt\").read()\n",
        "\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "input_sequences=[]\n",
        "\n",
        "for line in corpus:\n",
        "  token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "  for i in range(1,len(token_list)):\n",
        "    n_gram_sequence = token_list[:i+1]\n",
        "    input_sequences.append(n_gram_sequence)\n",
        "\n",
        "max_sequence_len = max((len(x) for x in input_sequences))\n",
        "input_sequences = np.array(sequence.pad_sequences(input_sequences,maxlen = max_sequence_len,padding='pre'))\n",
        "print(input_sequences.shape)\n",
        "\n",
        "predictors,label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "label = tf.keras.utils.to_categorical(label,num_classes=total_words)\n",
        "n = len(predictors)\n",
        "\n",
        "def train_val_test_split(predictors,label): \n",
        "  x_train = predictors[:(int)(0.7*n)]\n",
        "  y_train = label[:(int)(0.7*n)]\n",
        "  x_val = predictors[(int)(0.7*n):(int)(0.8*n)]\n",
        "  y_val = label[(int)(0.7*n):(int)(0.8*n)]\n",
        "  x_test = predictors[(int)(0.8*n):n]\n",
        "  y_test = label[(int)(0.8*n):n]\n",
        "  return x_train,y_train,x_val,y_val,x_test,y_test\n",
        "\n",
        "x_train,y_train,x_val,y_val,x_test,y_test = train_val_test_split(predictors,label)\n",
        "print(predictors.shape)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "metadata": {
        "id": "GJRwEHNS1qNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a2a3d2e-4e9e-4d0a-92dc-46b66701fa88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20205, 17)\n",
            "(20205, 16)\n",
            "(14143, 16)\n",
            "(4041, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(Embedding(total_words,100, input_length = max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
        "model.add(Attention(return_sequences=False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(total_words/2, activation='relu'))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsMESi38J1xb",
        "outputId": "2206a8c7-607a-4bb9-ebf4-911cf81e0d37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300, 36) (300, 36) (None, 16, 300)\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_12 (Embedding)    (None, 16, 100)           261500    \n",
            "                                                                 \n",
            " bidirectional_12 (Bidirecti  (None, 16, 300)          301200    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " attention_10 (Attention)    (None, 16, 36)            32400     \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 16, 36)            0         \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 576)               0         \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 1307)              754139    \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 2615)              3420420   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,769,659\n",
            "Trainable params: 4,769,659\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history3d=model.fit(predictors, label,\n",
        "          batch_size=100,\n",
        "          epochs=60, validation_data = [x_val,y_val],verbose=1)\n",
        "# print(history3d.history['loss'])\n",
        "# print(history3d.history['accuracy'])"
      ],
      "metadata": {
        "id": "sM7c8ioM5Lhk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "770ac6c9-8ccd-4cf8-ecb3-432e5332bed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n",
            "(300, 36) (300, 36) (None, 16, 300)\n",
            "(300, 36) (300, 36) (None, 16, 300)\n",
            "203/203 [==============================] - ETA: 0s - loss: 6.2615 - accuracy: 0.0496(300, 36) (300, 36) (None, 16, 300)\n",
            "203/203 [==============================] - 11s 30ms/step - loss: 6.2615 - accuracy: 0.0496 - val_loss: 5.9750 - val_accuracy: 0.0559\n",
            "Epoch 2/60\n",
            "203/203 [==============================] - 5s 27ms/step - loss: 5.9108 - accuracy: 0.0543 - val_loss: 5.7170 - val_accuracy: 0.0609\n",
            "Epoch 3/60\n",
            "203/203 [==============================] - 5s 25ms/step - loss: 5.7089 - accuracy: 0.0592 - val_loss: 5.6455 - val_accuracy: 0.0693\n",
            "Epoch 4/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 5.5426 - accuracy: 0.0658 - val_loss: 5.3894 - val_accuracy: 0.0732\n",
            "Epoch 5/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 5.3477 - accuracy: 0.0818 - val_loss: 5.1380 - val_accuracy: 0.0940\n",
            "Epoch 6/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 5.1272 - accuracy: 0.1032 - val_loss: 4.9371 - val_accuracy: 0.1237\n",
            "Epoch 7/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 4.9066 - accuracy: 0.1218 - val_loss: 4.6721 - val_accuracy: 0.1445\n",
            "Epoch 8/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 4.6758 - accuracy: 0.1398 - val_loss: 4.4045 - val_accuracy: 0.1633\n",
            "Epoch 9/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 4.4620 - accuracy: 0.1564 - val_loss: 4.1718 - val_accuracy: 0.1821\n",
            "Epoch 10/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 4.2251 - accuracy: 0.1732 - val_loss: 4.0124 - val_accuracy: 0.1950\n",
            "Epoch 11/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 3.9803 - accuracy: 0.1925 - val_loss: 3.6971 - val_accuracy: 0.2241\n",
            "Epoch 12/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 3.7187 - accuracy: 0.2128 - val_loss: 3.4111 - val_accuracy: 0.2514\n",
            "Epoch 13/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 3.4483 - accuracy: 0.2373 - val_loss: 3.1139 - val_accuracy: 0.2989\n",
            "Epoch 14/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 3.1961 - accuracy: 0.2693 - val_loss: 2.7946 - val_accuracy: 0.3508\n",
            "Epoch 15/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 2.9099 - accuracy: 0.3118 - val_loss: 2.5311 - val_accuracy: 0.4072\n",
            "Epoch 16/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 2.6481 - accuracy: 0.3624 - val_loss: 2.2447 - val_accuracy: 0.4478\n",
            "Epoch 17/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 2.3874 - accuracy: 0.4123 - val_loss: 2.0756 - val_accuracy: 0.4953\n",
            "Epoch 18/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 2.1717 - accuracy: 0.4530 - val_loss: 1.7704 - val_accuracy: 0.5547\n",
            "Epoch 19/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 1.9652 - accuracy: 0.4970 - val_loss: 1.6128 - val_accuracy: 0.5849\n",
            "Epoch 20/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 1.7675 - accuracy: 0.5385 - val_loss: 1.4398 - val_accuracy: 0.6205\n",
            "Epoch 21/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 1.6208 - accuracy: 0.5722 - val_loss: 1.3107 - val_accuracy: 0.6576\n",
            "Epoch 22/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 1.4696 - accuracy: 0.6051 - val_loss: 1.1661 - val_accuracy: 0.7016\n",
            "Epoch 23/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 1.3208 - accuracy: 0.6425 - val_loss: 1.0617 - val_accuracy: 0.7190\n",
            "Epoch 24/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 1.2095 - accuracy: 0.6694 - val_loss: 0.9929 - val_accuracy: 0.7486\n",
            "Epoch 25/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 1.1669 - accuracy: 0.6784 - val_loss: 0.8649 - val_accuracy: 0.7625\n",
            "Epoch 26/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 1.0440 - accuracy: 0.7085 - val_loss: 0.7806 - val_accuracy: 0.7788\n",
            "Epoch 27/60\n",
            "203/203 [==============================] - 5s 24ms/step - loss: 0.9648 - accuracy: 0.7308 - val_loss: 0.7611 - val_accuracy: 0.7942\n",
            "Epoch 28/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 0.9187 - accuracy: 0.7431 - val_loss: 0.7401 - val_accuracy: 0.8021\n",
            "Epoch 29/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 0.8444 - accuracy: 0.7630 - val_loss: 0.6370 - val_accuracy: 0.8238\n",
            "Epoch 30/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 0.7891 - accuracy: 0.7747 - val_loss: 0.5851 - val_accuracy: 0.8372\n",
            "Epoch 31/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 0.7622 - accuracy: 0.7871 - val_loss: 0.5504 - val_accuracy: 0.8506\n",
            "Epoch 32/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 0.7270 - accuracy: 0.7957 - val_loss: 0.5168 - val_accuracy: 0.8540\n",
            "Epoch 33/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 0.6529 - accuracy: 0.8130 - val_loss: 0.4885 - val_accuracy: 0.8699\n",
            "Epoch 34/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 0.6068 - accuracy: 0.8253 - val_loss: 0.4420 - val_accuracy: 0.8857\n",
            "Epoch 35/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 0.6048 - accuracy: 0.8262 - val_loss: 0.4639 - val_accuracy: 0.8639\n",
            "Epoch 36/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 0.6046 - accuracy: 0.8256 - val_loss: 0.4480 - val_accuracy: 0.8808\n",
            "Epoch 37/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 0.5697 - accuracy: 0.8383 - val_loss: 0.4146 - val_accuracy: 0.8877\n",
            "Epoch 38/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 0.5929 - accuracy: 0.8308 - val_loss: 0.3980 - val_accuracy: 0.8872\n",
            "Epoch 39/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 0.5056 - accuracy: 0.8555 - val_loss: 0.3798 - val_accuracy: 0.8956\n",
            "Epoch 40/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 0.4773 - accuracy: 0.8623 - val_loss: 0.3584 - val_accuracy: 0.8961\n",
            "Epoch 41/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 0.4674 - accuracy: 0.8627 - val_loss: 0.3336 - val_accuracy: 0.9099\n",
            "Epoch 42/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 0.4816 - accuracy: 0.8597 - val_loss: 0.3934 - val_accuracy: 0.8867\n",
            "Epoch 43/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 0.4736 - accuracy: 0.8634 - val_loss: 0.3440 - val_accuracy: 0.9040\n",
            "Epoch 44/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 0.4921 - accuracy: 0.8580 - val_loss: 0.3647 - val_accuracy: 0.8986\n",
            "Epoch 45/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 0.4397 - accuracy: 0.8711 - val_loss: 0.3272 - val_accuracy: 0.9040\n",
            "Epoch 46/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 0.4076 - accuracy: 0.8825 - val_loss: 0.2995 - val_accuracy: 0.9139\n",
            "Epoch 47/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 0.4276 - accuracy: 0.8757 - val_loss: 0.2986 - val_accuracy: 0.9085\n",
            "Epoch 48/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 0.4186 - accuracy: 0.8771 - val_loss: 0.3353 - val_accuracy: 0.9035\n",
            "Epoch 49/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 0.4013 - accuracy: 0.8812 - val_loss: 0.3094 - val_accuracy: 0.9075\n",
            "Epoch 50/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 0.4351 - accuracy: 0.8747 - val_loss: 0.3051 - val_accuracy: 0.9114\n",
            "Epoch 51/60\n",
            "203/203 [==============================] - 5s 24ms/step - loss: 0.3744 - accuracy: 0.8887 - val_loss: 0.2839 - val_accuracy: 0.9159\n",
            "Epoch 52/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 0.4157 - accuracy: 0.8801 - val_loss: 0.2969 - val_accuracy: 0.9099\n",
            "Epoch 53/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 0.3653 - accuracy: 0.8920 - val_loss: 0.3051 - val_accuracy: 0.9179\n",
            "Epoch 54/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 0.3793 - accuracy: 0.8885 - val_loss: 0.2827 - val_accuracy: 0.9164\n",
            "Epoch 55/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 0.3635 - accuracy: 0.8909 - val_loss: 0.2560 - val_accuracy: 0.9193\n",
            "Epoch 56/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 0.3454 - accuracy: 0.8964 - val_loss: 0.2487 - val_accuracy: 0.9238\n",
            "Epoch 57/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 0.3608 - accuracy: 0.8926 - val_loss: 0.2823 - val_accuracy: 0.9109\n",
            "Epoch 58/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 0.3680 - accuracy: 0.8905 - val_loss: 0.2733 - val_accuracy: 0.9149\n",
            "Epoch 59/60\n",
            "203/203 [==============================] - 5s 24ms/step - loss: 0.4264 - accuracy: 0.8754 - val_loss: 0.2907 - val_accuracy: 0.9124\n",
            "Epoch 60/60\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 0.3492 - accuracy: 0.8941 - val_loss: 0.2999 - val_accuracy: 0.9099\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"Drops of rain\"\n",
        "next_words = 10\n",
        "for _ in range(next_words):\n",
        "  token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "  token_list = sequence.pad_sequences([token_list],maxlen = max_sequence_len-1,padding='pre')\n",
        "  predicted = np.argmax(model.predict(token_list,verbose=0), axis=-1)\n",
        "  # print(predicted)\n",
        "  output_word = \"\"\n",
        "  for word,index in tokenizer.word_index.items():\n",
        "    if index == predicted:\n",
        "      # print(word)\n",
        "      output_word = word\n",
        "      break\n",
        "  seed_text+=\" \"+output_word\n",
        "\n",
        "print(seed_text)\n",
        "\n",
        "score, acc = model.evaluate(x_test, y_test,batch_size=100)\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "L2koznGWT3pL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2766b912-d372-4da5-c9a5-500718cd4409"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300, 36) (300, 36) (None, 16, 300)\n",
            "Drops of rain could be heard hitting the pane which made him feel\n",
            "41/41 [==============================] - 1s 13ms/step - loss: 0.1558 - accuracy: 0.9431\n",
            "41/41 [==============================] - 0s 11ms/step - loss: 0.2701 - accuracy: 0.9183\n",
            "0.9183370471000671\n"
          ]
        }
      ]
    }
  ]
}